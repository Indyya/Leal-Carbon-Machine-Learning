# -*- coding: utf-8 -*-
"""Leal Carbon Machine Learning Case Study

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EgfnSH_2qNS1R1HSbRD5chVZOODADhVx

# Leal Carbon Machine Learning Case Study

### About the Data set
The Global Carbon Project (GCP) is an in-depth dataset that looks into international CO2 emissions at the country level, dating back to 2001 (Andrew & Peters, 2022). The PDF describes the history of this process leading up to the technique used within the 2022 release of the GCP’s fossil CO2 emissions dataset, displaying how countries make a contribution to climate change on an international level. It is valuable to researchers, policymakers, and everyone inquisitive about the impact that human activities have on carbon emissions (Andrew & Peters, 2022).

###Size and Fortmat

This a very large dataset, 13.1 MB with 63103 rows × 11 columns.  Many times throughout this exercise, I will be shortening the length of dataset for to get more relavent information.

###Relevant Features


1.   Country: The name of the country.
2.   ISO 3166-1 alpha-3: The three-letter code for the country.
3. Year: The year of the data.
4. Total: The total amount of CO2 emissions for the country in the given year.
5. Coal: The amount of CO2 emissions from coal for the country in the given year.
6. Oil: The amount of CO2 emissions from oil for the country in the given year.
7. Gas: The amount of CO2 emissions from gas for the country in the given year.
8. Cement: The amount of CO2 emissions from cement production for the country in the given year.
9. Flaring: The amount of CO2 emissions from flaring operations for the country in the given year.
10. Other: The amount of CO2 emissions from other sources for the country in the given year.
11. Per Capita: The amount of CO2 emissions per capita for the country in the given year.

### Why use this data set?

  This dataset provides real-world data regarding fossil CO2 emissions and can be used to give LEAL Carbon reliable and accurate information. It also avoids any biases and focuses on insights from an objective viewpoint, aiding LEAL Carbon in its goal to reject extremism. Since the Global Carbon Project (GCP) covers emissions from various countries and sectors, LEAL Carbon can use this to create an inclusive environment. It's easier to make an AI-powered platform to empower individuals when you have a dataset that can identify effective and practical sustainability strategies that contribute to a greener future. If LEAL Carbon decides to use this dataset, It could aid in promoting sustainable choices to the users and provide useful information through personalized recommendations

###Data Cleaning, Preparation, Preprossing, etc
In this first selection of this code, I import the appropriate libraries for data analysis, data visualization, machine learning, and time series analysis. At this stage of the process, I'm only looking for basic information about the dataset so I can check for any missing values, identify patterns, etc.
"""

!pip install tensorflow-addons==0.16.1

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import os
from sklearn.preprocessing import MinMaxScaler, StandardScaler
import tensorflow as tf
import tensorflow_addons as tfa
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
import seaborn as sns
import plotly.express as px
import plotly.offline as po
import plotly.graph_objs as pg
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import cross_val_score

# Replace the file path with the actual path to your CSV file
file_path_MtCO2 = "/content/drive/MyDrive/Fossil C02 emissions dataset/GCB2022v27_MtCO2_flat.csv"

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path_MtCO2, skiprows=[1])

# display the first few rows of the DataFrame:
df

df.info()

df.describe()

# Getting the sum or mean of a column:
total_emissions_sum = df["Total"].sum()
total_emissions_sum

# Get the average per capita emissions:
average_per_capita = df["Per Capita"].mean()
average_per_capita

"""##Handling missing Values with KNN
K-nearest neighbors (KNN) imputation is used for numeric columns with missing data to fill in the missing values based on the values of their K-nearest neighbors. This technique leverages the similarity between data points to impute missing values(Hulten, 2018).

After doing this, the successful handling of missing values through KNN imputation ensures that there is no data loss due to missing information, and it allows us to utilize the entire dataset for accurate analysis and model training.


"""

# Step 1: Identify columns with missing values and assess the extent of missing data.
missing_count = df.isnull().sum()
print("Columns with missing values:")
missing_count

#For columns with minimal missing values, consider imputation using mean, median, or mode imputation.
minimal_missing_cols = ['Year']
for col in minimal_missing_cols:
    if df[col].isnull().sum() > 0:
        imputer = SimpleImputer(strategy='mean')
        df[col] = imputer.fit_transform(df[[col]])

# For non-numeric columns, use the 'most_frequent' strategy for imputation
non_numeric_cols = ['Country', 'ISO 3166-1 alpha-3']
for col in non_numeric_cols:
    if df[col].isnull().sum() > 0:
        imputer = SimpleImputer(strategy='most_frequent')
        df[col] = imputer.fit_transform(df[[col]])

# For numeric columns with missing data, use the 'mean' strategy for imputation
numeric_cols = ['Total', 'Coal', 'Oil', 'Gas', 'Cement', 'Flaring', 'Other', 'Per Capita']
for col in numeric_cols:
    if df[col].isnull().sum() > 0:
      #Advanced imputation using regression imputation or K-nearest neighbors imputation
        imputer = SimpleImputer(strategy='mean')
        df[col] = imputer.fit_transform(df[[col]])

print("\nNaN values after handling missing values:")
df.head(10)

"""##Handle the categorical variables
 One-hot encoding is a process used to convert categorical variables into a binary representation, which is suitable for machine learning algorithms that require numerical inputs(Hulten, 2018).

The one-hot encoding process has expanded the dataset by creating new binary features for each unique category in the "Country" and "ISO 3166-1 alpha-3" columns. The new DataFrame, df_encoded, now contains binary representations of the categorical data, making it suitable for various machine learning algorithms that require numerical inputs.
"""

# Identify the categorical columns that need to be encoded
categorical_cols = ["Country", "ISO 3166-1 alpha-3"]
df_encoded = pd.get_dummies(df)

# Applies one-hot encoding to the entire DataFrame
print("\nDataFrame after one-hot encoding:")
df_encoded

"""##Handling the outliers

Here, I am performing outlier detection on the "Total" column of the dataset after filtering the data to include values between 1950 and 2021 for more relvent information.

The summary statistics shows that the mean value of the "Total" column is 181.06, the median is 2.73, and the standard deviation is 1576.80. The quartile values are Q1 = 0.21 and Q3 = 30.01, resulting in an IQR of 29.80. The potential outliers are identified based on the IQR method, which helps flag extreme values that are beyond the lower and upper bounds calculated using the IQR.




"""

# Filter data to include values between 1950 and 2021
df_filtered = df[(df["Year"] >= 1950) & (df["Year"] <= 2021)]

# Calculate summary statistics
mean_value = df_filtered["Total"].mean()
median_value = df_filtered["Total"].median()
standard_deviation = df_filtered["Total"].std()

# Calculate quartiles and IQR
Q1 = df_filtered["Total"].quantile(0.25)
Q3 = df_filtered["Total"].quantile(0.75)
IQR = Q3 - Q1

# Identify potential outliers using the IQR method
lower_outlier_bound = Q1 - 1.5 * IQR
upper_outlier_bound = Q3 + 1.5 * IQR

potential_outliers = df_filtered[
    (df_filtered["Total"] < lower_outlier_bound) |
    (df_filtered["Total"] > upper_outlier_bound)
]

# Display the results
print("Mean:", mean_value)
print("Median:", median_value)
print("Standard Deviation:", standard_deviation)
print("Q1:", Q1)
print("Q3:", Q3)
print("IQR:", IQR)
print("Potential Outliers:")
potential_outliers.head(10)

"""##Scaling and Normalization

This part of the code normalizes the DataFrame by scaling to numeric columns in the DataFrame 'df' to ensure that all numerical features have a consistent range suitable for machine learning algorithms. It applied Min-Max scaling, which transformed each feature to a specific range between 0 and 1, preserving the relative relationships between data points.

As a result, all the numeric features now fall within the range of 0 to 1, making them comparable and ensuring that their magnitudes do not unduly influence machine learning models. By scaling the features, the data is prepared for modeling tasks, and potential issues related to varied ranges and outlier influence are mitigated.


"""

# Identifying Numeric Columns
num_cols = ['Total', 'Coal', 'Oil', 'Gas', 'Cement', 'Flaring', 'Other', 'Per Capita']
num_feats = df[num_cols]

# Check if all columns are numeric (float or int)
num_feats = num_feats.apply(pd.to_numeric)

# Check for constant or low variance columns
low_variance_cols = num_feats.columns[num_feats.var() < 1e-5]
print(f"Low Variance Columns: {low_variance_cols}")

# Remove low variance columns from num_feats if any
num_feats = num_feats.drop(columns=low_variance_cols)

# Apply MinMaxScaler with feature_range=(0, 1)
scaler = MinMaxScaler(feature_range=(0, 1))
num_scaled = scaler.fit_transform(num_feats)

# Convert the scaled values back to a DataFrame
num_scaled_df = pd.DataFrame(num_scaled, columns=num_feats.columns)
num_scaled_df

"""## Feature Selection

Here, I am performing feature selection using a RandomForestRegressor model to identify the most important features for predicting the 'Total' emissions, which is assumed to be the target variable.(Hulten, 2018)The feature importance values provide insights into which features contribute the most to the variation in 'Total' emissions.


What I gathered is that the 'Oil', 'Flaring', 'Gas', 'Coal', and 'Year' features are identified as the top 5 important features for predicting the 'Total' emissions. These features have the highest impact on the target variable according to the RandomForestRegressor model(Hulten, 2018).

"""

# Assuming 'Total' is the target variable for prediction
target = 'Total'

# Separate the features and target variable
X = df.drop(columns=[target, 'Country'])  # Drop 'Country' column
y = df[target]

# Convert the 'ISO 3166-1 alpha-3' column to numerical format using one-hot encoding
X_encoded = pd.get_dummies(X, columns=['ISO 3166-1 alpha-3'])

# Convert all columns to numeric format
X_encoded = X_encoded.apply(pd.to_numeric, errors='coerce')

# Perform data imputation for missing values in 'X_encoded'
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_encoded)

# Check and handle missing values in 'y' (target variable)
if y.isnull().any():
    y = y.fillna(y.mean())

# Feature Importance using RandomForest
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_imputed, y)
feature_importance = rf_model.feature_importances_
print("Feature Importance from RandomForest:")
feature_importance

# Sort feature importance scores in descending order
sorted_importance_indices = feature_importance.argsort()[::-1]

# Get the names of the top 5 features
top_k_features = X_encoded.columns[sorted_importance_indices[:5]]

print("Top 5 important features:")
print(top_k_features)

"""
##Visualization

Now let's conduct an analysis of carbon emissions data, specifically focusing on various fuel categories and exploring trends in emissions over the years. I did this through displaying fuel category maps, top 10 means and pie charts, top 10 years by per capita and total emission, correlation heatmap, and trend analysis.

Before getting into the charts, I prepare the dataset by removing irrelevant years and focusing on the years after 1950. I also handle missing values in the ISO codes to make sure the dataset is suitable for visualization and analysis.



"""

#dropping years less than 1950
df.drop(df[df['Year'] < 1950].index, inplace = True)

#removing the values of the world
glob= df[df['ISO 3166-1 alpha-3'] == 'WLD']

#dropping missing iso values columns
df.drop(df[df['ISO 3166-1 alpha-3'] == 'WLD'].index, inplace = True)

"""###Choropleth maps
The choropleth maps show the countries with the highest average emissions for each fuel category. This helps identify which countries are major contributors to specific types of emissions.

I calculated the mean emission for each fuel category in each country. I then choose the top 100 countries with the highest mean emissions per category and made individual choropleth maps for each fuel category, displaying the countries' average emissions
"""

def display_fuel_category_maps(df):
    fuel_categories = ['Coal', 'Oil', 'Gas', 'Cement', 'Flaring', 'Other', 'Total']
    fuel_means = {}
    for fuel in fuel_categories:
        fuel_mean_df = df.groupby(['ISO 3166-1 alpha-3', 'Country'])[fuel].mean().nlargest(100).reset_index()
        fuel_means[fuel] = fuel_mean_df
    for fuel in fuel_categories:
        fig = px.choropleth(fuel_means[fuel],
                            locations="ISO 3166-1 alpha-3",
                            color=fuel,
                            hover_name="Country",
                            color_continuous_scale=px.colors.sequential.Plasma,
                            title=f"Top 100 Countries by {fuel} Emission")
        fig.show()
display_fuel_category_maps(df)

"""###Pie Chart

Here I created Pie Charts that highlight the distribution of emissions among the top 10 countries. We can see these countries' contributions when it comes to emissions for each category, as well as Per Capita emissions, for the past 10 years.

The dominance of countries like the united states of America, Russia, China, the UK, Japan, and India have the highest emissions, and consumption of coal, oil, gasoline, cement, and flaring may be attributed to a mixture of things(Evans, 2022). First off, those international locations are essential industrial and financial powerhouses with high power demands. As a result, they heavily rely on coal, oil, and gasoline for strength generation, industrial tactics, and transportation(Evans, 2022). Their big production sectors and transportation systems force up the demand for those fossil fuels, leading to higher emissions.

After viewing the charts, certain countries show up repeatedly due to the fact these countries are the main commercial and monetary powerhouses with high energy needs. For instance, look at the first pie chart. Concerning coal emissions, China is number one. China is considered one of the biggest producers AND customers of coal due to its coal-fired energy plant life for power generation and heavy industries. While countries like the US and Russia own plentiful natural resources of oil and gas, they considerably to global emissions and may even result in flaring(IEA,2021).

Another factor could be urbanization and infrastructure development that had been increasing over the past 10 years. China and India are examples of this and as a result, a substantial demand for cement followed suit.


"""

columns_to_mean = ['Coal', 'Oil', 'Gas', 'Cement', 'Flaring', 'Other', 'Total', 'Per Capita']
top_10_means = {}
for column in columns_to_mean:
    top_10_means[column] = df.groupby(['ISO 3166-1 alpha-3', 'Country'])[column].mean().nlargest(10).reset_index()
for column in columns_to_mean:
    fig = px.pie(top_10_means[column], values=column, names='Country', title=f"Top 10 Countries by {column} Emission")
    fig.show()
if 'Internal Transport' in df.columns:
    df.drop('Internal Transport', axis=1, inplace=True)
top_10_years_per_capita = df.groupby(['Year'])['Per Capita'].mean().nlargest(10).reset_index()
top_10_years_total = df.groupby(['Year'])['Total'].mean().nlargest(10).reset_index()
fig_pc = px.pie(top_10_years_per_capita, values='Per Capita', names='Year', title="Top 10 Years by Per Capita Emission")
fig_pc.show()
fig_total = px.pie(top_10_years_total, values='Total', names='Year', title="Top 10 Years by Total Emission")
print(fig_total.show())

"""###Correlation heatmap
The correlation heatmap allows for the observation of relationships between different emission types, helping understand whether certain types of emissions are positively or negatively correlated(Hulten, 2018).

Let's look at some of the negative corrlations in the heatmap:

X: Year Y: Per Capita Value: -0.00082
1. This could imply that as years progress, countries may be taking actions to reduce their per capita emissions(Ötker-Robe, 2014).

X: Year Y: Other Value: -0.077
2. It might be due to advancements in technology, policy changes, or shifts in energy sources, which have resulted in lower emissions in this category(Bank for International Settlements, n.d.).

X: Other Y: Per Capita Value: -0.02
3. It could imply that countries with higher 'Other' emissions might have lower per capita emissions due to factors like population size and total emissions(Jorgenson & Clark, 2013).

"""

plt.figure(figsize=(12, 6))
sns.heatmap(df.corr(), cmap='YlGnBu', annot=True)
plt.show()

"""### Line plots
The trend analysis using line plots demonstrates how emissions, both 'Per Capita' and 'Total,' have evolved over the years, providing insights into emission trends and fluctuations.
"""

trend_pc = df.groupby(['Year'])['Per Capita'].mean().reset_index()
trend_total = df.groupby(['Year'])['Total'].mean().reset_index()

# Trend of total emission over the years
total_emissions = px.line(trend_pc, x="Year", y="Per Capita")
total_emissions.update_xaxes(rangeslider_visible=True)
total_emissions.show()

# Trend of per capita emission over the years
per_capita_emission = px.line(trend_total, x="Year", y="Total")
per_capita_emission.update_xaxes(rangeslider_visible=True)
per_capita_emission.show()

"""##Model Development

###Linear Regression

This helped to  predict carbon emissions (the target variable) based on various features (independent variables) present in the dataset. The selected features include factors like population, economic indicators (GDP), emissions from different sources like coal, oil, gas, cement, flaring, and other sources, as well as per capita emissions.

The high R-squared value (approximately 0.9876) indicates that the model can explain around 98.76% of the variance in carbon emissions using the selected features(Hulten, 2018). This suggests that the chosen features are strongly correlated with the target variable (emissions), and the model has learned a meaningful relationship between them. It has captured the underlying patterns and trends in the data, which enables it to make accurate predictions of emissions for unseen instances.

The relatively low Mean Squared Error (MSE) of approximately 2459.08 further supports the model's effectiveness in predicting emissions. The MSE measures the average squared difference between the predicted and actual emissions. A low MSE indicates that the model's predictions are generally close to the actual values, signifying good performance.
"""

# Split the data into features (X) and target (y)
X = df.drop(columns=["Country", "ISO 3166-1 alpha-3", "Total"])
y = df["Total"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Linear Regression model
model = LinearRegression()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R-squared:", r2)

"""###Alternative Evaluation Metrics
Calculate and compare alternative evaluation metrics like Mean Absolute Error (MAE) and root mean squared error (RMSE).
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("Mean Absolute Error:", mae)
print("Root Mean Squared Error:", rmse)

"""###Regularization
See if it improves generalization.
"""

from sklearn.linear_model import Ridge

ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train, y_train)
y_pred_ridge = ridge_model.predict(X_test)

r2_ridge = r2_score(y_test, y_pred_ridge)
print("R-squared (Ridge):", r2_ridge)

"""###Checking for overfitting

To ensure that the model is not overfitting, capturing random noise/fluctuations instead of the patterns and relationships, I checked for overfitting. The similar R-squared scores between the initial Linear Regression and the cross-validation evaluation indicate that the model is not overfitting and is generalizing well to unseen data.
"""

# Create a Linear Regression model
model = LinearRegression()

# Perform 5-fold cross-validation and calculate R-squared scores
r2_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')

# Calculate the average R-squared score
average_r2 = np.mean(r2_scores)

print("Average R-squared score:", average_r2)

"""###Time Clustering

I calculated the mean emissions of each country by their total then I use PCA, Principal Component Analysis, to reduce the data by the dimensionality of the mean emission data. The goal is to get only the important variations in the data using PCA component 1. Afterward, I clustered using the K-means algorithm into 5 clusters with different emission patterns.  

The last thing I did is Anomaly Detecting with the Isolation Forest algorithm so I can spot the countries with unusual emission behaviors that deviate significantly from the majority of countries.

The 'cluster countries' show how some countries are grouped in comparison to others while the 'Anomaly countries' show which countries have unusually high emission behaviors, high or low.
"""

# 'Total' column contains the target variable for clustering
target_column = 'Total'
target_series = df[target_column]

# Group the data by country and calculate the mean emissions for each country
country_mean_emissions = df.groupby('Country')[target_column].mean().reset_index()

# Perform PCA to reduce dimensionality
pca = PCA(n_components=1)  # Only one component for PCA
country_mean_emissions_pca = pca.fit_transform(country_mean_emissions[target_column].values.reshape(-1, 1))

# Use K-means clustering to cluster countries based on emission patterns
n_clusters = 5
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters_country = kmeans.fit_predict(country_mean_emissions_pca)

# Define cluster labels
cluster_labels = {
    0: "Low Emission Countries",
    1: "Moderate Emission Countries",
    2: "High Emission Countries",
    3: "Fluctuating Emission Countries",
    4: "Other Emission Pattern"
}

# Add cluster labels to the country_mean_emissions DataFrame
country_mean_emissions['Cluster'] = [cluster_labels[label] for label in clusters_country]

# Print the cluster labels for each country without repeats
cluster_df = country_mean_emissions[['Country', 'Cluster']].drop_duplicates()
cluster_df

# Assuming 'Country' column is present in the dataset
countries = df['Country']

# Normalize the time series data
scaler = StandardScaler()
target_series_normalized = scaler.fit_transform(target_series.values.reshape(-1, 1))

# Perform PCA to reduce dimensionality (using a single component)
pca = PCA(n_components=1)  # Only one component for PCA
target_series_pca = pca.fit_transform(target_series_normalized)

# Use K-means clustering to cluster countries based on emission patterns
n_clusters = 5
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
clusters_target = kmeans.fit_predict(target_series_pca)

# Plot the clustered countries
plt.figure(figsize=(10, 6))
for i in range(n_clusters):
    cluster_data = target_series_pca[clusters_target == i]
    plt.scatter(cluster_data, np.zeros_like(cluster_data), label= cluster_labels[i])
plt.xlabel('PCA Component 1')
plt.title('Time Series Clustering of Countries based on Emission Patterns')
plt.legend()
plt.show()

# Add the cluster labels back to the dataframe
df['Cluster Target'] = clusters_target

# Use Isolation Forest for anomaly detection
isolation_forest = IsolationForest(contamination=0.05)  # Adjust contamination percentage as needed
anomaly_labels = isolation_forest.fit_predict(target_series_normalized)

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest

# Assuming 'Country' column is present in the dataset
countries = df['Country']

# Normalize the time series data
scaler = StandardScaler()
target_series_normalized = scaler.fit_transform(df['Total'].values.reshape(-1, 1))

# Perform PCA to reduce dimensionality (using a single component)
pca = PCA(n_components=1)  # Only one component for PCA
target_series_pca = pca.fit_transform(target_series_normalized)

# Use K-means clustering to cluster countries based on emission patterns
n_clusters = 5
kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)  # Set n_init explicitly to suppress the warning
clusters_target = kmeans.fit_predict(target_series_pca)

# Add the cluster labels back to the dataframe
df['Cluster Target'] = clusters_target

# Create a new DataFrame 'country_mean_emissions' to group by 'Country' and calculate mean emissions
country_mean_emissions = df.groupby('Country')['Total'].mean().reset_index()

# Use Isolation Forest for anomaly detection on 'country_mean_emissions'
isolation_forest = IsolationForest(contamination=0.05)  # Adjust contamination percentage as needed
anomaly_labels = isolation_forest.fit_predict(country_mean_emissions['Total'].values.reshape(-1, 1))

# Add the anomaly labels to the 'country_mean_emissions' DataFrame
country_mean_emissions['Anomaly'] = np.where(anomaly_labels == -1, 'Anomaly', 'Normal')

# Filter out countries with anomaly (label = 'Anomaly')
anomaly_countries = country_mean_emissions[country_mean_emissions['Anomaly'] == 'Anomaly']

# Drop duplicates to keep only one occurrence of each country
anomaly_countries.drop_duplicates(subset='Country', keep='first', inplace=True)

# Print the countries with unusual emission behavior
print("Countries with Unusual Emission Behavior:")
anomaly_countries[['Country', 'Anomaly']]

"""The results show that the list of anomalies countries is consistent with that of the top 10 emitters for the past 10 years. That means that these countries have a huge, stable impact on the carbon footprint due to industrialization, economic activities, energy production, and population growth(Ötker-Robe, 2014). An issue this wide scale may call for International Cooperation, as a collective effort is needed to adopt more ambitious emission reduction goals. Collective efforts, regular assessing, and proactive measures must be taken in order to see any sort of change.

## Source

1. Andrew, R. M., &amp; Peters, G. P. (2022, October 17). The Global Carbon Project’s Fossil CO2 Emissions Dataset. Zenodo. https://zenodo.org/record/7215364
2. Bank for International Settlements. (n.d.). Basel Committee on Banking Supervision - Bank for International Settlements. bis. https://www.bis.org/bcbs/publ/d517.pdf
3. Evans, S. (2022, May 12). Analysis: Which countries are historically responsible for climate change?. Carbon Brief. https://www.carbonbrief.org/analysis-which-countries-are-historically-responsible-for-climate-change/
4. Hulten, G. (2018). Building Intelligent Systems A Guide to Machine Learning Engineering. Apress.
5. IEA (2021), Greenhouse Gas Emissions from Energy Data Explorer, IEA, Paris https://www.iea.org/data-and-statistics/data-tools/greenhouse-gas-emissions-from-energy-data-explorer
5. Jorgenson, A. K., &amp; Clark, B. (2013, February 20). The relationship between national-level carbon dioxide emissions and population size: An assessment of regional and temporal variation, 1960-2005. PloS one. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3577780/
6. Saier, A. (2022, October 26). Climate Plans Remain Insufficient: More Ambitious Action Needed Now. Unfccc.int. https://unfccc.int/news/climate-plans-remain-insufficient-more-ambitious-action-needed-now
7. Ötker-Robe, İ. (2014, October). Global Risks and Collective Action Failures: What Can the International Community Do?. imf. https://www.imf.org/external/pubs/ft/wp/2014/wp14195.pdf
"""